# DevPilot â€” Technical Specification
**AI-Augmented Developer Workflow Framework**
Version: 0.1 Draft | Tool Target: Claude Code / Cursor

---

## 1. Overview

DevPilot is a CLI-first, agent-orchestrated developer workflow framework that automates the path from issue â†’ code â†’ test â†’ PR â†’ merge with clearly defined human supervision gates. It is built to be dropped into any existing repo with a single config file.

**Primary Build Environment:** Claude Code (recommended) or Cursor Agent Mode
**Primary Runtime:** Python 3.11+, runs in CI or locally
**Deployment Target:** GitHub Actions, GitLab CI, or self-hosted runners

---

## 2. Guiding Architecture Principles

- **Agent-per-concern** â€” each agent has one job; no god agents
- **Config-first** â€” everything controlled from `devpilot.yaml`; no hardcoded logic
- **Confidence-gated** â€” agents self-score; low-confidence tasks escalate to humans automatically
- **Observable by default** â€” every agent action is logged, traced, and reviewable
- **Escape hatches everywhere** â€” humans can override any automated decision at any stage

---

## 3. Repository Structure

```
devpilot/
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                  # Entry: `devpilot <command>`
â”‚   â”œâ”€â”€ commands/
â”‚   â”‚   â”œâ”€â”€ init.py              # `devpilot init` â€” scaffold config + CI
â”‚   â”‚   â”œâ”€â”€ run.py               # `devpilot run` â€” trigger full pipeline
â”‚   â”‚   â”œâ”€â”€ agent.py             # `devpilot agent <name>` â€” run single agent
â”‚   â”‚   â””â”€â”€ status.py            # `devpilot status` â€” show pipeline state
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ base.py                  # BaseAgent class (LLM calls, logging, confidence scoring)
â”‚   â”œâ”€â”€ planner.py               # Issue â†’ task breakdown
â”‚   â”œâ”€â”€ coder.py                 # Task â†’ code implementation
â”‚   â”œâ”€â”€ reviewer.py              # Diff â†’ review comments + approve/reject
â”‚   â”œâ”€â”€ tester.py                # Code â†’ test generation + execution
â”‚   â””â”€â”€ debugger.py              # Failure log â†’ root cause â†’ patch
â”‚
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ graph.py                 # LangGraph state machine â€” pipeline definition
â”‚   â”œâ”€â”€ state.py                 # PipelineState dataclass
â”‚   â”œâ”€â”€ checkpoints.py           # Human gate logic + notification dispatch
â”‚   â””â”€â”€ router.py               # Conditional edge logic (pass/fail/escalate)
â”‚
â”œâ”€â”€ integrations/
â”‚   â”œâ”€â”€ github/
â”‚   â”‚   â”œâ”€â”€ client.py            # PyGitHub wrapper
â”‚   â”‚   â”œâ”€â”€ pr.py                # PR creation, labeling, review
â”‚   â”‚   â””â”€â”€ issues.py            # Issue fetch, comment, close
â”‚   â”œâ”€â”€ sentry.py                # Error ingestion + issue creation
â”‚   â”œâ”€â”€ jira.py                  # Optional: ticket sync
â”‚   â””â”€â”€ slack.py                 # Notification dispatch
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ code_executor.py         # Safe subprocess wrapper (test runner)
â”‚   â”œâ”€â”€ file_editor.py           # Read/write/patch files (used by coder agent)
â”‚   â”œâ”€â”€ git_ops.py               # Branch, commit, push wrappers
â”‚   â””â”€â”€ static_analysis.py      # Lint, type-check, complexity scoring
â”‚
â”œâ”€â”€ supervision/
â”‚   â”œâ”€â”€ gates.py                 # Gate definitions + approval logic
â”‚   â”œâ”€â”€ confidence.py            # Confidence score computation
â”‚   â””â”€â”€ audit_log.py             # Immutable append-only decision log
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ ci/
â”‚   â”‚   â”œâ”€â”€ github-actions.yaml  # Generated workflow file
â”‚   â”‚   â””â”€â”€ gitlab-ci.yaml
â”‚   â”œâ”€â”€ prompts/                 # All agent system + user prompts (versioned)
â”‚   â”‚   â”œâ”€â”€ planner.md
â”‚   â”‚   â”œâ”€â”€ coder.md
â”‚   â”‚   â”œâ”€â”€ reviewer.md
â”‚   â”‚   â”œâ”€â”€ tester.md
â”‚   â”‚   â””â”€â”€ debugger.md
â”‚   â””â”€â”€ configs/
â”‚       â””â”€â”€ devpilot.yaml        # Template config for `devpilot init`
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”‚
â”œâ”€â”€ devpilot.yaml                # User's project config (generated by init)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## 4. Core Configuration: `devpilot.yaml`

```yaml
# devpilot.yaml â€” drop this in your repo root

project:
  name: "my-service"
  language: "python"           # python | typescript | go | java
  test_command: "pytest"
  lint_command: "ruff check ."
  main_branch: "main"

llm:
  provider: "anthropic"
  model: "claude-opus-4-6"     # or claude-sonnet-4-6 for speed/cost
  fallback_model: "claude-sonnet-4-6"
  max_tokens: 8192
  temperature: 0.2             # Low for code, slightly higher for planning

agents:
  planner:
    enabled: true
    max_subtasks: 10
  coder:
    enabled: true
    max_iterations: 3          # Retry limit before escalation
    style_guide: ".cursor/rules" # or path to CLAUDE.md / .cursorrules
  reviewer:
    enabled: true
    auto_approve_threshold: 0.92   # Confidence score 0-1
    block_on: ["security", "data_loss"]
  tester:
    enabled: true
    frameworks: ["pytest", "jest"]
    coverage_threshold: 80
    generate_on: ["new_file", "modified_function"]
  debugger:
    enabled: true
    max_fix_attempts: 3
    sources: ["ci_logs", "sentry", "test_output"]

supervision:
  gates:
    - id: "pre-merge-staging"
      trigger: "before_merge_to_staging"
      type: "human_approval"
      notify: ["slack", "email"]
      timeout_hours: 24
      on_timeout: "block"      # block | auto_approve | escalate

    - id: "pre-merge-main"
      trigger: "before_merge_to_main"
      type: "human_approval"
      notify: ["slack", "email"]
      required_approvers: 1
      timeout_hours: 8

    - id: "confidence-gate"
      trigger: "on_low_confidence"
      threshold: 0.75
      type: "human_approval"
      notify: ["slack"]

  audit_log: ".devpilot/audit.jsonl"

integrations:
  github:
    enabled: true
    auto_label_prs: true
    pr_template: ".github/pull_request_template.md"
    labels:
      ai_generated: "ðŸ¤– ai-generated"
      needs_review: "ðŸ‘€ needs-review"
  sentry:
    enabled: false
    project_slug: "my-service"
    auto_create_issues: true
  slack:
    enabled: true
    channel: "#dev-pipeline"
  jira:
    enabled: false

pipeline:
  # Defines the default flow; can be overridden per-trigger
  flow:
    - planner
    - coder
    - reviewer
    - tester
    - gate:pre-merge-staging
    - gate:pre-merge-main

  triggers:
    - on: "github_issue"
      label: "ai-task"
      flow: [planner, coder, reviewer, tester, gate:pre-merge-staging]
    - on: "ci_failure"
      flow: [debugger, tester, reviewer]
    - on: "sentry_alert"
      flow: [debugger, coder, tester, gate:pre-merge-staging]
    - on: "schedule"
      cron: "0 2 * * *"       # Nightly: run tester + reviewer on open PRs
      flow: [tester, reviewer]
```

---

## 5. Agent Specifications

### 5.1 BaseAgent

```python
# agents/base.py
class BaseAgent:
    def __init__(self, config: AgentConfig, state: PipelineState):
        self.config = config
        self.state = state
        self.client = anthropic.Anthropic()
        self.logger = AuditLogger()

    def run(self, input: AgentInput) -> AgentOutput:
        raise NotImplementedError

    def call_llm(self, system: str, messages: list, tools: list = []) -> Message:
        # Wraps API call with retry, logging, token tracking
        ...

    def score_confidence(self, output: str, rubric: dict) -> float:
        # Ask LLM to self-score its output against rubric
        # Returns 0.0 - 1.0
        ...

    def escalate(self, reason: str) -> EscalationEvent:
        # Logs escalation, notifies human, blocks pipeline
        ...
```

### 5.2 Planner Agent

**Input:** GitHub Issue (title, body, labels, linked PRs)
**Output:** Structured task list with file hints and acceptance criteria

**Prompt strategy:**
- System: `templates/prompts/planner.md` â€” instructs structured JSON output
- Tools: `github.issues.get_context()`, `codebase_search()`
- Output schema:
```json
{
  "tasks": [
    {
      "id": "T1",
      "title": "Add rate limiting middleware",
      "files_likely_affected": ["src/middleware/", "tests/middleware/"],
      "acceptance_criteria": ["Returns 429 after N requests", "Configurable via env"],
      "estimated_complexity": "medium",
      "dependencies": []
    }
  ],
  "confidence": 0.88,
  "blockers": []
}
```

### 5.3 Coder Agent

**Input:** Single task from Planner output + full file context
**Output:** File diffs, new files, git commit

**Key behaviors:**
- Reads existing code style before writing (respects `.cursorrules` / `CLAUDE.md`)
- Writes code â†’ runs lint â†’ fixes lint errors â†’ re-runs (up to `max_iterations`)
- Uses `file_editor.py` and `git_ops.py` as tools
- Never touches files outside declared `files_likely_affected` without re-planning

**Tools available to Coder:**
```python
tools = [
    read_file(path),
    write_file(path, content),
    run_command(cmd),       # lint, type check only â€” not tests (Tester's job)
    search_codebase(query),
    git_diff(),
    git_commit(message)
]
```

### 5.4 Reviewer Agent

**Input:** Git diff of coder's changes
**Output:** Review decision + structured comments

**Review rubric (used for confidence scoring):**
- Correctness: does it solve the task?
- Security: no obvious vulns (SQL injection, secrets in code, etc.)
- Style: matches existing patterns
- Test coverage: adequate coverage added?
- Side effects: does it break existing behavior?

**Output schema:**
```json
{
  "decision": "approve" | "request_changes" | "escalate_to_human",
  "confidence": 0.91,
  "comments": [
    {
      "file": "src/api/routes.py",
      "line": 42,
      "severity": "warning" | "blocking",
      "body": "Missing input validation on user_id parameter"
    }
  ],
  "summary": "Changes look correct. One non-blocking style concern."
}
```

If `decision == "request_changes"`, coder agent gets another iteration.
If `confidence < auto_approve_threshold`, always escalate to human gate.

### 5.5 Tester Agent

**Input:** Changed files + existing test suite
**Output:** New/updated test files, test run results

**Behaviors:**
- Detects test framework from config + existing files
- Generates tests targeting changed functions specifically (not full coverage padding)
- Runs tests via `code_executor.py`
- On failure: passes failure log to Debugger Agent
- Reports coverage delta (must meet `coverage_threshold`)

### 5.6 Debugger Agent

**Input:** Failure log (CI output, Sentry event, test output) + relevant source files
**Output:** Root cause analysis + patch

**Chain of thought approach (prompt instructs explicit steps):**
1. Parse error type and stack trace
2. Identify the failing code path
3. Hypothesize root cause (top 3 ranked by likelihood)
4. Generate minimal fix for top hypothesis
5. Re-run tests to verify fix
6. If still failing: try next hypothesis or escalate

---

## 6. Orchestration: LangGraph State Machine

```python
# orchestrator/graph.py

from langgraph.graph import StateGraph, END
from orchestrator.state import PipelineState

def build_pipeline(config: DevPilotConfig) -> StateGraph:
    graph = StateGraph(PipelineState)

    # Add nodes
    graph.add_node("planner",  PlannerAgent(config).run)
    graph.add_node("coder",    CoderAgent(config).run)
    graph.add_node("reviewer", ReviewerAgent(config).run)
    graph.add_node("tester",   TesterAgent(config).run)
    graph.add_node("debugger", DebuggerAgent(config).run)
    graph.add_node("gate",     HumanGate(config).check)
    graph.add_node("pr",       PRCreator(config).create)

    # Entry point set by trigger type
    graph.set_entry_point("planner")

    # Conditional edges
    graph.add_conditional_edges("planner", route_planner, {
        "proceed": "coder",
        "escalate": "gate",
        "abort": END
    })

    graph.add_conditional_edges("reviewer", route_reviewer, {
        "approve": "tester",
        "request_changes": "coder",   # loops back with comments
        "escalate": "gate",
    })

    graph.add_conditional_edges("tester", route_tester, {
        "pass": "gate",
        "fail": "debugger",
        "coverage_fail": "coder",
    })

    graph.add_conditional_edges("debugger", route_debugger, {
        "fixed": "tester",            # re-runs tests
        "max_retries": "gate",
        "abort": END
    })

    graph.add_conditional_edges("gate", route_gate, {
        "approved": "pr",
        "rejected": END,
        "timeout": END
    })

    graph.add_edge("pr", END)
    return graph.compile()
```

**PipelineState:**
```python
@dataclass
class PipelineState:
    trigger: TriggerEvent
    issue: Optional[GitHubIssue]
    tasks: list[Task]
    current_task_idx: int
    iterations: dict[str, int]       # agent_name -> retry count
    confidence_scores: dict[str, float]
    review_comments: list[ReviewComment]
    test_results: Optional[TestResult]
    patch: Optional[str]
    pr_url: Optional[str]
    gate_decisions: dict[str, GateDecision]
    audit_entries: list[AuditEntry]
    status: PipelineStatus
```

---

## 7. Human Supervision Gates

```python
# supervision/gates.py

class HumanGate:
    """
    Blocks pipeline and waits for human approval.
    Supports Slack interactive buttons, GitHub PR review, or CLI approval.
    """

    def check(self, state: PipelineState) -> GateDecision:
        gate_config = self.get_gate_config(state)

        # Notify human
        self.notify(gate_config, state)

        # Poll for approval (with timeout)
        decision = self.poll_for_decision(
            gate_id=gate_config.id,
            timeout_hours=gate_config.timeout_hours
        )

        self.audit_log.record(gate_config.id, decision, state)
        return decision

    def notify(self, gate: GateConfig, state: PipelineState):
        summary = self.build_summary(state)  # LLM-generated human-readable summary

        if "slack" in gate.notify:
            self.slack.send_approval_request(
                channel=self.config.slack.channel,
                summary=summary,
                approve_url=self.generate_approval_url(gate.id),
                reject_url=self.generate_rejection_url(gate.id),
                context_url=state.pr_url
            )

        if "github" in gate.notify:
            # Add required reviewer to PR
            self.github.request_review(state.pr_url, gate.required_approvers)
```

**Gate types:**
- `human_approval` â€” blocks until explicit approve/reject
- `confidence_gate` â€” auto-blocks when agent confidence < threshold
- `time_gate` â€” auto-proceeds after N hours if no action (configurable)
- `branch_gate` â€” different rules for different target branches

---

## 8. CI/CD Integration

### GitHub Actions Template (generated by `devpilot init`)

```yaml
# .github/workflows/devpilot.yaml (auto-generated)

name: DevPilot AI Pipeline

on:
  issues:
    types: [labeled]
  workflow_dispatch:
    inputs:
      trigger_type:
        type: choice
        options: [issue, ci_failure, sentry_alert]

jobs:
  devpilot:
    runs-on: ubuntu-latest
    if: github.event.label.name == 'ai-task'

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install DevPilot
        run: pip install devpilot

      - name: Run Pipeline
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DEVPILOT_SLACK_TOKEN: ${{ secrets.SLACK_TOKEN }}
        run: |
          devpilot run \
            --trigger github_issue \
            --issue-number ${{ github.event.issue.number }} \
            --config devpilot.yaml
```

---

## 9. CLI Interface

```bash
# Initialize DevPilot in a project
devpilot init
  --language python
  --ci github-actions
  --integrations github,slack

# Run full pipeline from issue
devpilot run --trigger github_issue --issue 42

# Run a single agent manually
devpilot agent coder --task "Add input validation to /api/users"
devpilot agent tester --files src/api/users.py
devpilot agent debugger --log ci_failure.txt

# Show current pipeline status
devpilot status

# Approve a pending gate (from CLI instead of Slack)
devpilot gate approve --id pre-merge-staging
devpilot gate reject --id pre-merge-staging --reason "Needs security review"

# View audit log
devpilot audit --last 20
devpilot audit --pipeline-id abc123
```

---

## 10. Prompt Engineering Conventions

All prompts live in `templates/prompts/` as versioned Markdown files. Each follows this structure:

```markdown
# [Agent Name] System Prompt (v1.2)

## Role
You are the [role] in an automated software development pipeline...

## Capabilities
- Tool 1: description
- Tool 2: description

## Output Format
Always respond with valid JSON matching this schema: ...

## Constraints
- Never modify files outside the declared scope
- If confidence < 0.75, set escalate: true
- ...

## Reasoning Protocol
Before taking action:
1. State your understanding of the task
2. List files you will read
3. List changes you will make
4. Estimate confidence
```

**Why external prompts (not hardcoded strings):**
- Version controllable
- A/B testable without code changes
- User-overridable: place custom prompts in `.devpilot/prompts/`

---

## 11. Observability & Audit Log

Every agent action writes a structured entry:

```jsonl
{"ts":"2026-02-27T10:00:00Z","pipeline":"abc123","agent":"coder","action":"write_file","file":"src/api/users.py","tokens_used":1842,"confidence":0.91,"duration_ms":4200}
{"ts":"2026-02-27T10:01:00Z","pipeline":"abc123","agent":"reviewer","action":"decision","result":"request_changes","comments":2,"confidence":0.85}
{"ts":"2026-02-27T10:05:00Z","pipeline":"abc123","gate":"pre-merge-staging","action":"approved","approver":"@jane","method":"slack"}
```

View with `devpilot audit` or stream to Langfuse for full LLM tracing.

---

## 12. Tech Stack

| Layer | Technology |
|---|---|
| CLI | Python + Typer |
| Agent orchestration | LangGraph |
| LLM calls | Anthropic Python SDK (`claude-opus-4-6`) |
| GitHub integration | PyGitHub + webhook receiver (FastAPI) |
| Tool integrations | Composio or custom MCP servers |
| LLM observability | Langfuse |
| Config parsing | Pydantic v2 + PyYAML |
| Code execution sandbox | subprocess with timeout + Docker option |
| Notifications | Slack Bolt SDK |
| Testing | pytest + pytest-asyncio |
| Packaging | pyproject.toml + pip-installable |

---

## 13. Build Order (Claude Code / Cursor Execution Plan)

Use this sequence as your Claude Code task list or Cursor agent instructions:

**Phase 1 â€” Core Skeleton (Week 1)**
1. `devpilot init` CLI command + `devpilot.yaml` schema (Pydantic models)
2. `BaseAgent` class with Anthropic SDK integration + audit logger
3. `PipelineState` dataclass + LangGraph graph scaffold (no real agents yet â€” stubs)
4. `devpilot run` that executes the stub graph end-to-end

**Phase 2 â€” Agents (Week 2)**
5. Planner Agent + prompt + unit tests
6. Coder Agent + `file_editor.py` + `git_ops.py` tools
7. Reviewer Agent + confidence scoring
8. Tester Agent + `code_executor.py` sandbox
9. Debugger Agent + failure log parser

**Phase 3 â€” Integrations & Gates (Week 3)**
10. GitHub integration (issues, PRs, labels)
11. Human gate system (Slack approval flow)
12. CI/CD template generation (`devpilot init --ci github-actions`)
13. Sentry integration (optional)

**Phase 4 â€” Hardening (Week 4)**
14. Integration tests with a real sample repo
15. Langfuse observability wiring
16. Prompt tuning pass across all agents
17. README + `devpilot init` interactive wizard

---

## 14. Claude Code Usage Notes

When building with **Claude Code**, structure your `CLAUDE.md` like this:

```markdown
# DevPilot CLAUDE.md

## Project
Python CLI tool for AI-augmented dev workflows.

## Commands
- `pytest` â€” run tests
- `ruff check .` â€” lint
- `mypy devpilot/` â€” type check

## Architecture Rules
- All LLM calls go through `BaseAgent.call_llm()` â€” never call SDK directly in agent logic
- All file writes go through `file_editor.py` â€” never use `open()` directly in agents
- Prompts live in `templates/prompts/` â€” never hardcode prompts as strings in Python
- New agents must subclass `BaseAgent` and implement `run(input) -> output`

## Test Strategy
- Unit test agents with mocked LLM responses
- Integration tests use fixtures in `tests/fixtures/` (real file trees, real diffs)
```

When building with **Cursor**, place equivalent rules in `.cursor/rules/devpilot.mdc`.

---

## 15. Open Questions / Decisions Needed

- **Multi-repo support:** Should pipelines span multiple repos (monorepo vs polyrepo)?
- **Model routing:** Use Opus for planner/reviewer, Sonnet for coder/tester to manage cost?
- **Self-hosted vs SaaS gate approvals:** Slack is easy; do you need a web dashboard?
- **Security model:** How are `ANTHROPIC_API_KEY` and secrets managed across orgs?
- **Language priority:** Python-first or TypeScript-first for v1?
